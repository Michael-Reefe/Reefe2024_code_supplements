{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c89d8",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a6ac",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# MIRI MRS Batch Processing Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fb85c",
   "metadata": {},
   "source": [
    "**Author**: David Law, AURA Associate Astronomer, MIRI branch\n",
    "<br>\n",
    "**Last Updated**: June 29, 2023\n",
    "<br>\n",
    "**Pipeline Version**: 1.11.0\n",
    "\n",
    "Customized by Michael Reefe, October 18, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988f765",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a framework for batch processing of MIRI MRS data through all three pipeline stages.  Data is assumed to be located in two Observation folders (science and background) according to paths set up below.\n",
    "\n",
    "This setup has been modified to produce the best quailty output data products that we have found for the observations of the Phoenix Cluster.  This includes enabling strict cosmic ray flagging, residual fringe correction, and 2D background subtraction, in addition to extra custom routines for flagging bad pixels and subtracting stirpe artifacts from cosmic rays.\n",
    "\n",
    "Changes:<br>\n",
    "Sep 1 2022: Add some commentary and example on how to use multicore processing in Detector 1<br>\n",
    "Sep 12 2022: Disable unnecessary cube/1d spectra production for individual science exposures in Spec 2<br>\n",
    "Oct 14 2022: Include residual fringe correction in spec2 (note that this will CRASH earlier pipeline versions!<br>\n",
    "Jun 29 2023: Update to latest 1.11.0 pipeline with photom, outlier detection, and x1d changes, add CRDS path options.  Change to SMP LMC 058 demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9868",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec45d0",
   "metadata": {},
   "source": [
    "1.<font color='white'>-</font>Configuration <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to be changed here.\n",
    "# It should not be necessary to edit cells below this in general unless modifying pipeline processing steps.\n",
    "\n",
    "import sys,os, pdb\n",
    "\n",
    "# CRDS context (if overriding)\n",
    "#%env CRDS_CONTEXT jwst_1093.pmap\n",
    "\n",
    "# Set CRDS paths if not set already in your .bashrc shell configuration\n",
    "#os.environ['CRDS_PATH']='/Users/dlaw/crds_cache'\n",
    "#os.environ['CRDS_SERVER_URL']='https://jwst-crds.stsci.edu'\n",
    "# Echo CRDS path in use\n",
    "print('CRDS local filepath:',os.environ['CRDS_PATH'])\n",
    "print('CRDS file server:',os.environ['CRDS_SERVER_URL'])\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the science observation\n",
    "input_dir = 'Phoenix_uncal/'\n",
    "\n",
    "# Point to where you want the output science results to go\n",
    "output_dir = 'Phoenix_cal/'\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the background observation\n",
    "# If no background observation, leave this blank\n",
    "input_bgdir = 'Phoenix_uncal_bg/'\n",
    "\n",
    "# Point to where the output background observations should go\n",
    "# If no background observation, leave this blank\n",
    "# output_bgdir = ''\n",
    "output_bgdir = 'Phoenix_cal_bg/'\n",
    "\n",
    "# Whether or not to process only data from a given band/channel\n",
    "# Useful if overriding reference files\n",
    "# Note BOTH must be set in order to work\n",
    "use_ch='' # '12' or '34'\n",
    "use_band='' # 'SHORT', 'MEDIUM', or 'LONG'\n",
    "\n",
    "# Whether or not to run a given pipeline stage\n",
    "# Science and background are processed independently through det1+spec2, and jointly in spec3\n",
    "\n",
    "# Science processing\n",
    "dodet1=True\n",
    "dospec2=True\n",
    "dospec3=True\n",
    "\n",
    "# Background processing\n",
    "dodet1bg=True\n",
    "dospec2bg=True\n",
    "\n",
    "# If there is no background folder, ensure we don't try to process it\n",
    "if (input_bgdir == ''):\n",
    "    dodet1bg=False\n",
    "    dospec2bg=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28d9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c3a015b",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c995",
   "metadata": {},
   "source": [
    "2.<font color='white'>-</font>Imports and setup <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the entire available screen width for the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7191bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "# Astropy utilities for opening FITS and ASCII files\n",
    "from astropy.io import fits\n",
    "from astropy.io import ascii\n",
    "from astropy.utils.data import download_file\n",
    "# Astropy utilities for making plots\n",
    "from astropy.visualization import (LinearStretch, LogStretch, ImageNormalize, ZScaleInterval)\n",
    "\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322125b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the base JWST package\n",
    "import jwst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JWST pipelines (encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels # JWST datamodels\n",
    "from jwst.associations import asn_from_list as afl # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base # Definition of a Lvl3 association file\n",
    "\n",
    "from stcal import dqflags # Utilities for working with the data quality (DQ) arrays\n",
    "import miricoord\n",
    "import miricoord.mrs.mrs_tools as mrstools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64fe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output subdirectories to keep science data products organized\n",
    "# Note that the pipeline might complain about this as it is intended to work with everything in a single\n",
    "# directory, but it nonetheless works fine for the examples given here.\n",
    "det1_dir = os.path.join(output_dir, 'stage1mod/') # Detector1 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(output_dir, 'stage2mod/') # Spec2 pipeline outputs will go here\n",
    "spec3_dir = os.path.join(output_dir, 'stage3mod/') # Spec3 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if not os.path.exists(det1_dir):\n",
    "    os.makedirs(det1_dir)\n",
    "if not os.path.exists(spec2_dir):\n",
    "    os.makedirs(spec2_dir)\n",
    "if not os.path.exists(spec3_dir):\n",
    "    os.makedirs(spec3_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "060e06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output subdirectories to keep background data products organized\n",
    "det1_bgdir = os.path.join(output_bgdir, 'stage1mod/') # Detector1 pipeline outputs will go here\n",
    "spec2_bgdir = os.path.join(output_bgdir, 'stage2mod/') # Spec2 pipeline outputs will go here\n",
    "spec3_bgdir = os.path.join(output_bgdir, 'stage3mod/') # Spec2 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if (output_bgdir != ''):\n",
    "    if not os.path.exists(det1_bgdir):\n",
    "        os.makedirs(det1_bgdir)\n",
    "    if not os.path.exists(spec2_bgdir):\n",
    "        os.makedirs(spec2_bgdir)\n",
    "    if not os.path.exists(spec3_bgdir):\n",
    "        os.makedirs(spec3_bgdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1d77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a7f86",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c9165",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>Detector1 Pipeline <a class=\"anchor\" id=\"det1\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through the Detector1 pipeline to create Lvl2a data products (i.e., uncalibrated slope images).\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ec2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll define a function that will call the detector1 pipeline with our desired set of parameters\n",
    "# We won't enumerate the individual steps\n",
    "def rundet1(filename, outdir):\n",
    "    print(filename)\n",
    "    if os.path.exists(os.path.join(outdir, os.path.basename(filename).replace('_uncal.fits', '_rate.fits'))) and \\\n",
    "        os.path.exists(os.path.join(outdir, os.path.basename(filename).replace('_uncal.fits', '_rateints.fits'))):\n",
    "        print('Output already exists! Skipping...')\n",
    "        return\n",
    "\n",
    "    det1 = Detector1Pipeline() # Instantiate the pipeline\n",
    "    det1.output_dir = outdir # Specify where the output should go\n",
    "    \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #det1.dq_init.skip = False\n",
    "    #det1.saturation.skip = False\n",
    "    #det1.firstframe.skip = False\n",
    "    #det1.lastframe.skip = False\n",
    "    #det1.reset.skip = False\n",
    "    #det1.linearity.skip = False\n",
    "    #det1.rscd.skip = False\n",
    "    #det1.dark_current.skip = False\n",
    "    #det1.refpix.skip = False\n",
    "    #det1.jump.skip = False\n",
    "    #det1.ramp_fit.skip = False\n",
    "    #det1.gain_scale.skip = False\n",
    "    # det1.ipc.skip = False\n",
    "    \n",
    "    # The jump and ramp fitting steps can benefit from multi-core processing, but this is off by default\n",
    "    # Turn them on here if desired by choosing how many cores to use (quarter, half, or all)\n",
    "    det1.jump.maximum_cores='half'\n",
    "    # det1.jump.find_showers=True\n",
    "    # det1.jump.extend_snr_threshold=1.2  # (default = 1.2)\n",
    "    # det1.jump.sat_required_snowball=False\n",
    "    det1.ramp_fit.maximum_cores='half'\n",
    "    # This next parameter helps with very bright objects and/or very short ramps\n",
    "    # det1.jump.three_group_rejection_threshold=100\n",
    "\n",
    "    # JUMP overrides. \n",
    "    # Currently pipeline is not flagging lower-level jumps\n",
    "    # like we might want it to, so lower thresholds for more\n",
    "    # aggressive flagging.\n",
    "    #det1.jump.save_results = True\n",
    "    det1.jump.rejection_threshold = 3.5         # default 4.0sigma\n",
    "    det1.jump.min_jump_to_flag_neighbors = 8.0  # default 10sigma\n",
    "\n",
    "    # Additional JUMP overrides related to CR shower flagging. See\n",
    "    # JWST pipeline documentation page for details of parameters.\n",
    "    # https://jwst-pipeline.readthedocs.io/en/stable/jwst/jump/index.html\n",
    "    det1.jump.expand_large_events   = True   # Turn on shower flagging\n",
    "    det1.jump.use_ellipses          = True   # True for MIRI; approximate showers as elliptical\n",
    "    det1.jump.min_jump_area         = 12     # Min # of contiguous pixels to trigger expanded flagging\n",
    "    det1.jump.sat_required_snowball = False  # Do not require pixels to be saturated to flag\n",
    "    det1.jump.expand_factor         = 3.0    # expands showers beyond ID'd jumps; default 2.0\n",
    "    \n",
    "    det1.jump.after_jump_flag_dn1   = 10     # These 4 related to how long after a jump is identified\n",
    "    det1.jump.after_jump_flag_time1 = 20     #  we should keep flagging the following integrations\n",
    "    det1.jump.after_jump_flag_dn2   = 1000\n",
    "    det1.jump.after_jump_flag_time2 = 3000\n",
    "    \n",
    "    # Bad pixel mask overrides\n",
    "    #det1.dq_init.override_mask = 'myfile.fits'\n",
    "\n",
    "    # Saturation overrides\n",
    "    #et1.saturation.override_saturation = 'myfile.fits'\n",
    "    \n",
    "    # Reset overrides\n",
    "    #det1.reset.override_reset = 'myfile.fits'\n",
    "        \n",
    "    # Linearity overrides\n",
    "    #det1.linearity.override_linearity = 'myfile.fits'\n",
    "\n",
    "    # RSCD overrides\n",
    "    #det1.rscd.override_rscd = 'myfile.fits'\n",
    "        \n",
    "    # DARK overrides\n",
    "    #det1.dark_current.override_dark = 'myfile.fits'\n",
    "        \n",
    "    # GAIN overrides\n",
    "    #det1.jump.override_gain = 'myfile.fits'\n",
    "    #det1.ramp_fit.override_gain = 'myfile.fits'\n",
    "                \n",
    "    # READNOISE overrides\n",
    "    #det1.jump.override_readnoise = 'myfile.fits'\n",
    "    #det1.ramp_fit.override_readnoise = 'myfile.fits'\n",
    "        \n",
    "    det1.save_results = True # Save the final resulting _rate.fits files\n",
    "    det1(filename) # Run the pipeline on an input list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f129790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the science observation\n",
    "sstring = input_dir + 'jw*mirifu*uncal.fits'\n",
    "lvl1b_files = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(lvl1b_files))\n",
    "    for ii in range(0,len(lvl1b_files)):\n",
    "        hdu=fits.open(lvl1b_files[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    lvl1b_files=lvl1b_files[indx]\n",
    "\n",
    "print('Found ' + str(len(lvl1b_files)) + ' science input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "if dodet1:\n",
    "    for file in lvl1b_files:\n",
    "        rundet1(file, det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the background observation\n",
    "sstring = input_bgdir + 'jw*mirifu*uncal.fits'\n",
    "lvl1b_files = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(lvl1b_files))\n",
    "    for ii in range(0,len(lvl1b_files)):\n",
    "        hdu=fits.open(lvl1b_files[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    lvl1b_files=lvl1b_files[indx]\n",
    "\n",
    "print('Found ' + str(len(lvl1b_files)) + ' background input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "if dodet1bg:\n",
    "    for file in lvl1b_files:\n",
    "        rundet1(file, det1_bgdir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d984a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41541b35",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe0fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad pixel mask refinement. Apologies that this code is not the most\n",
    "# well-organized or prettiest....\n",
    "dobadpix = True\n",
    "\n",
    "if dobadpix:\n",
    "    # This is a long command that sets... wait for it... dnubit = 1.\n",
    "    dnubit = dqflags.interpret_bit_flags('DO_NOT_USE',mnemonic_map=datamodels.dqflags.pixel)\n",
    "    \n",
    "    # We will start with the LONG detector. Grab the background exposures.\n",
    "    longrates = sorted(glob.glob(det1_bgdir+'*mirifulong*rate.fits'))\n",
    "    nlong = len(longrates)\n",
    "    \n",
    "    # Assemble background exposures\n",
    "    longstack = np.zeros([nlong,1024,1032])\n",
    "    for i,exposure in enumerate(longrates):\n",
    "        hdu = fits.open(exposure)\n",
    "        longstack[i,:,:] = hdu['SCI'].data\n",
    "        hdu.close()\n",
    "    \n",
    "    # Take the median of the exposures, then fix any pixels where the median is nan\n",
    "    longmedian = np.median(longstack,axis=0)\n",
    "    fillvalue  = np.nanmedian(longmedian)\n",
    "    print('Long stack fill value = {0:.3f}'.format(fillvalue))\n",
    "    \n",
    "    dqlong = (fits.open(longrates[0]))['DQ'].data\n",
    "    bad = np.where((dqlong & dnubit) != 0)\n",
    "    longmedian[bad] = fillvalue\n",
    "    \n",
    "    # Now write our median'd background to disk.\n",
    "    hdu = fits.PrimaryHDU(longmedian)\n",
    "    hdu.writeto(det1_bgdir+'background_median_mirifulong.fits',overwrite=True)\n",
    "    \n",
    "    \n",
    "    # And the SHORT detector\n",
    "    shortrates = sorted(glob.glob(det1_bgdir+'*mirifushort*rate.fits'))\n",
    "    nshort = len(shortrates)\n",
    "    \n",
    "    # Assemble background exposures\n",
    "    shortstack = np.zeros([nshort,1024,1032])\n",
    "    for i,exposure in enumerate(shortrates):\n",
    "        hdu = fits.open(exposure)\n",
    "        shortstack[i,:,:] = hdu['SCI'].data\n",
    "        hdu.close()\n",
    "    \n",
    "    # Take the median of the exposures, then fix any pixels where the median is nan\n",
    "    shortmedian = np.median(shortstack,axis=0)\n",
    "    fillvalue  = np.nanmedian(shortmedian)\n",
    "    print('Short stack fill value = {0:.3f}'.format(fillvalue))\n",
    "    \n",
    "    dqshort = (fits.open(shortrates[0]))['DQ'].data\n",
    "    bad = np.where((dqshort & dnubit) != 0)\n",
    "    shortmedian[bad] = fillvalue\n",
    "    \n",
    "    # Now write our median'd background to disk.\n",
    "    hdu = fits.PrimaryHDU(shortmedian)\n",
    "    hdu.writeto(det1_bgdir+'background_median_mirifushort.fits',overwrite=True)    \n",
    "    \n",
    "\n",
    "    # Now we have to identify the bad pixels based on their percentile value, but\n",
    "    # since each channel has different pixel distributions (e.g. Ch4 has very low\n",
    "    # throughput), we need to ID which pixels belong to which channel. This uses\n",
    "    # the distortion files from the miricoord package.\n",
    "\n",
    "    # Start again with the long exposures...\n",
    "    # Note in this folder\n",
    "    # MIRIFULONG vs MIRIFUSHORT = the two different detectors\n",
    "    # 12SHORT / 12MEDIUM / 12LONG = channels 1 & 2 for band A=short B=medium C=long\n",
    "    # 34SHORT / 34MEDIUM / 34LONG = channels 3 & 4 for band A=short B=medium C=long\n",
    "    # so really having \"34\" and MIRIFULONG in the names is redundant, but whatever...\n",
    "    miricoordpath = '/'.join(miricoord.__file__.split('/')[:-2])\n",
    "    longslicemask  = miricoordpath + '/data/fits/flt3/MIRI_FM_MIRIFULONG_34LONG_DISTORTION_9B.05.07.fits'\n",
    "    longslicemask  = (fits.open(longslicemask))['SLICE_NUMBER'].data[0,:,:]\n",
    "    shortslicemask = miricoordpath + '/data/fits/flt3/MIRI_FM_MIRIFUSHORT_12LONG_DISTORTION_9B.05.07.fits'\n",
    "    shortslicemask = (fits.open(shortslicemask))['SLICE_NUMBER'].data[0,:,:]\n",
    "    ch1 = (shortslicemask >= 100) & (shortslicemask <= 130)\n",
    "    ch2 = (shortslicemask >= 200) & (shortslicemask <= 230)\n",
    "    ch3 = (longslicemask >= 300) & (longslicemask <= 330)\n",
    "    ch4 = (longslicemask >= 400) & (longslicemask <= 430)\n",
    "    \n",
    "    # Get wavelength map for channel 4C. Since throughput falls so rapidly\n",
    "    # towards the longer Ch4 wavelengths, we will calculate the background median\n",
    "    # in wavelength chunks. Ch3 is way less affected by this so we just do it in\n",
    "    # one fell swoop.\n",
    "    wave4c = mrstools.waveimage('4C')\n",
    "    \n",
    "    # Subtract Ch3 average from Ch3 pixels\n",
    "    ch3pix = np.where(ch3)\n",
    "    ch3med = np.nanmedian(longmedian[ch3pix])\n",
    "    longmedian[ch3pix] = longmedian[ch3pix] - ch3med\n",
    "    print('For Ch3, median value was {0:.3f}'.format(ch3med))\n",
    "    \n",
    "    # The background in Ch4 varies strongly with wavelength so we have to subdivide it.\n",
    "    # Ch4C spans from 23.942 - 28.785 um (unclear why this is different than the \n",
    "    #  jdox page on MRS, 23.22 - 28.10 um.....). \n",
    "    # Let's split it into ~0.33um spans, subtract a wave-dependent background.\n",
    "    #mins = [ 1.0, 24.5, 25.0, 25.5, 26.0, 26.5, 27.0, 27.5, 28.0]\n",
    "    mins = [1.0, 24.33, 24.67, 25.0, 25.33, 25.67, 26.0, 26.33, 26.67, 27.0, 27.33, 27.67, 28.0, 28.33]\n",
    "    maxs = mins[1:] + [50.0]\n",
    "    for i,lowave in enumerate(mins):\n",
    "        hiwave = maxs[i]\n",
    "        ch4pix = np.where(ch4 & (wave4c > lowave) & (wave4c <= hiwave))\n",
    "        ch4med = np.nanmedian(longmedian[ch4pix])\n",
    "        longmedian[ch4pix] = longmedian[ch4pix] - ch4med\n",
    "        print('For wavelength range {0:.1f} - {1:.1f}um, found {2:.0f} pixels.'.format(lowave,hiwave,ch4pix[0].size))\n",
    "        print('  Median value was {0:.3f}'.format(ch4med))\n",
    "    \n",
    "    # Update the median image with a new fill value, write to disk again...\n",
    "    bad = np.where((dqlong & dnubit) != 0)\n",
    "    fillvalue = np.nanmedian(longmedian)\n",
    "    longmedian[bad] = fillvalue\n",
    "    print('Long stack re-fill value = {0:.3f}'.format(fillvalue))\n",
    "    hdu = fits.PrimaryHDU(longmedian)\n",
    "    hdu.writeto(det1_bgdir+'background_median_refill_mirifulong.fits',overwrite=True)    \n",
    "    \n",
    "    # Now we start ID'ing pixels to mask, starting with channel 3\n",
    "    longdqflags = np.zeros_like(dqlong)\n",
    "    \n",
    "    good = np.where(ch3 & (longdqflags == 0) & (np.isfinite(longmedian)))\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(111)\n",
    "    ax.plot(longmedian[good],'.',ms=0.5)\n",
    "    ax.set_ylim(-1,1)\n",
    "    ax.set_title('Channel 3 pixel values')\n",
    "    \n",
    "    # these values were identified 'by hand'; an alternative would be to\n",
    "    # make some kind of percentile-based or sigma-clipped threshold value\n",
    "    ch3lowcut, ch3hicut = -0.16, +0.10\n",
    "    \n",
    "    ax.axhline(ch3lowcut,color='k',ls='--')\n",
    "    ax.axhline(ch3hicut, color='k',ls='--')\n",
    "    plt.show()\n",
    "    plt.close(f)\n",
    "    \n",
    "    tomask = np.where(ch3 & (longmedian < ch3lowcut))\n",
    "    longdqflags[tomask] = 1\n",
    "    tomask = np.where(ch3 & (longmedian > ch3hicut))\n",
    "    longdqflags[tomask] = 1\n",
    "    \n",
    "    # Now do the same thing for Ch4\n",
    "    good = np.where(ch4 & (longdqflags == 0) & (np.isfinite(longmedian)))\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(111)\n",
    "    ax.plot(longmedian[good],'.',ms=0.5)\n",
    "    ax.set_ylim(-1,1)\n",
    "    ax.set_title('Channel 4 pixel values')\n",
    "    \n",
    "    # again ID'd cutoffs by hand from plot above\n",
    "    ch4lowcut, ch4hicut = -0.16, +0.27\n",
    "    \n",
    "    ax.axhline(ch4lowcut,color='k',ls='--')\n",
    "    ax.axhline(ch4hicut, color='k',ls='--')\n",
    "    plt.show()\n",
    "    plt.close(f)\n",
    "    \n",
    "    tomask = np.where(ch4 & (longmedian < ch4lowcut))\n",
    "    longdqflags[tomask] = 1\n",
    "    tomask = np.where(ch4 & (longmedian > ch4hicut))\n",
    "    longdqflags[tomask] = 1\n",
    "    \n",
    "    longmedian_clean = longmedian.copy()\n",
    "    bad = np.where(longdqflags == 1)\n",
    "    longmedian_clean[bad] = fillvalue\n",
    "    \n",
    "    # Save the final 'clean' background to disk for long-wave detector\n",
    "    hdu = fits.PrimaryHDU(longmedian_clean)\n",
    "    hdu.writeto(det1_bgdir+'background_median_clean_mirifulong.fits',overwrite=True)\n",
    "    \n",
    "    # Now apply correction to our bg + sci files.\n",
    "    longrates = sorted(glob.glob(det1_dir+'jw*mirifulong_rate.fits'))\n",
    "    for i,rate in enumerate(longrates):\n",
    "        hdu = fits.open(rate)\n",
    "        outfile = rate.replace('rate','ratemod')\n",
    "        dq = hdu['DQ'].data\n",
    "        dq[bad] = np.bitwise_or(dq[bad], dnubit)\n",
    "        hdu['DQ'].data = dq\n",
    "        hdu.writeto(outfile,overwrite=True)\n",
    "    longrates = sorted(glob.glob(det1_bgdir+'jw*mirifulong_rate.fits'))\n",
    "    for i,rate in enumerate(longrates):\n",
    "        hdu = fits.open(rate)\n",
    "        outfile = rate.replace('rate','ratemod')\n",
    "        dq = hdu['DQ'].data\n",
    "        dq[bad] = np.bitwise_or(dq[bad], dnubit)\n",
    "        hdu['DQ'].data = dq\n",
    "        hdu.writeto(outfile,overwrite=True)\n",
    "\n",
    "    # Now we repeat the above, but for Ch1 & 2 on the short-wave detector\n",
    "    # Because the background in Chs 1 & 2 is much lower and more uniform\n",
    "    # than 3&4, we don't need to process them separately and can just ID\n",
    "    # one set of bad pixel threshold cuts.\n",
    "    \n",
    "    # Subtract Ch1 average from Ch1 pixels\n",
    "    ch1pix = np.where(ch1)\n",
    "    ch1med = np.nanmedian(shortmedian[ch1pix])\n",
    "    shortmedian[ch1pix] = shortmedian[ch1pix] - ch1med\n",
    "    print('For Ch1, median value was {0:.3f}'.format(ch1med))\n",
    "    \n",
    "    # Subtract Ch2 average from Ch2 pixels\n",
    "    ch2pix = np.where(ch2)\n",
    "    ch2med = np.nanmedian(shortmedian[ch2pix])\n",
    "    shortmedian[ch2pix] = shortmedian[ch2pix] - ch2med\n",
    "    print('For Ch2, median value was {0:.3f}'.format(ch2med))\n",
    "    \n",
    "    # Update the median image with a new fill value, write to disk again...\n",
    "    bad = np.where((dqshort & dnubit) != 0)\n",
    "    fillvalue = np.nanmedian(shortmedian)\n",
    "    shortmedian[bad] = fillvalue\n",
    "    print('Short stack re-fill value = {0:.3f}'.format(fillvalue))\n",
    "    hdu = fits.PrimaryHDU(shortmedian)\n",
    "    hdu.writeto(det1_bgdir+'background_median_refill_mirifushort.fits',overwrite=True)    \n",
    "    \n",
    "    # Now we start ID'ing pixels to mask\n",
    "    shortdqflags = np.zeros_like(dqshort)\n",
    "    \n",
    "    good = np.where((shortdqflags == 0) & (np.isfinite(shortmedian)))\n",
    "    \n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(111)\n",
    "    ax.plot(shortmedian[good],'.',ms=0.5)\n",
    "    ax.set_ylim(-1,1)\n",
    "    ax.set_title('Channels 1&2 pixel values')\n",
    "\n",
    "    lowcut, hicut = -0.14, +0.13\n",
    "    \n",
    "    ax.axhline(lowcut,color='k',ls='--')\n",
    "    ax.axhline(hicut, color='k',ls='--')\n",
    "    plt.show()\n",
    "    plt.close(f)\n",
    "    \n",
    "    tomask = np.where((shortmedian < lowcut))\n",
    "    shortdqflags[tomask] = 1\n",
    "    tomask = np.where((longmedian > hicut))\n",
    "    shortdqflags[tomask] = 1\n",
    "\n",
    "    shortmedian_clean = shortmedian.copy()\n",
    "    bad = np.where(shortdqflags == 1)\n",
    "    shortmedian_clean[bad] = fillvalue\n",
    "    \n",
    "    hdu = fits.PrimaryHDU(shortmedian_clean)\n",
    "    hdu.writeto(det1_bgdir+'background_median_clean_mirifushort.fits',overwrite=True)\n",
    "    \n",
    "    # Now apply correction to our bg + sci files.\n",
    "    shortrates = sorted(glob.glob(det1_dir+'jw*mirifushort_rate.fits'))\n",
    "    for i,rate in enumerate(shortrates):\n",
    "        hdu = fits.open(rate)\n",
    "        outfile = rate.replace('rate','ratemod')\n",
    "        dq = hdu['DQ'].data\n",
    "        dq[bad] = np.bitwise_or(dq[bad], dnubit)\n",
    "        hdu['DQ'].data = dq\n",
    "        hdu.writeto(outfile,overwrite=True)\n",
    "    shortrates = sorted(glob.glob(det1_bgdir+'jw*mirifushort_rate.fits'))\n",
    "    for i,rate in enumerate(shortrates):\n",
    "        hdu = fits.open(rate)\n",
    "        outfile = rate.replace('rate','ratemod')\n",
    "        dq = hdu['DQ'].data\n",
    "        dq[bad] = np.bitwise_or(dq[bad], dnubit)\n",
    "        hdu['DQ'].data = dq\n",
    "        hdu.writeto(outfile,overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81419572",
   "metadata": {},
   "source": [
    "4.<font color='white'>-</font>Spec2 Pipeline <a class=\"anchor\" id=\"spec2\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through the Spec2 pipeline in order to produce Lvl2b data products (i.e., calibrated slope images and quick-look data cubes and 1d spectra).  \n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3e6a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl2 association file from an input list\n",
    "def writel2asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMSLevel2bBase, product_name=prodname)\n",
    "        \n",
    "    # Add background files to the association\n",
    "    nbg=len(bgfiles)\n",
    "    for ii in range(0,nbg):\n",
    "        asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b42d499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will call the spec2 pipeline with our desired set of parameters\n",
    "# We'll list the individual steps just to make it clear what's running\n",
    "def runspec2(filename, outdir, nocubes=False):\n",
    "    spec2 = Spec2Pipeline()\n",
    "    spec2.output_dir = outdir\n",
    "\n",
    "    # Assign_wcs overrides\n",
    "    #spec2.assign_wcs.override_distortion = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_regions = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_specwcs = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_wavelengthrange = 'myfile.asdf'\n",
    "\n",
    "    # Flatfield overrides\n",
    "    #spec2.flat.override_flat = 'myfile.fits'\n",
    "        \n",
    "    # Straylight overrides\n",
    "    #spec2.straylight.override_mrsxartcorr = 'myfile.fits'\n",
    "        \n",
    "    # Fringe overrides\n",
    "    #spec2.fringe.override_fringe = 'myfile.fits'\n",
    "    \n",
    "    # Photom overrides\n",
    "    #spec2.photom.override_photom = 'myfile.fits'\n",
    "\n",
    "    # Cubepar overrides\n",
    "    #spec2.cube_build.override_cubepar = 'myfile.fits'\n",
    "        \n",
    "    # Extract1D overrides\n",
    "    #spec2.extract_1d.override_extract1d = 'myfile.asdf'\n",
    "    #spec2.extract_1d.override_apcorr = 'myfile.asdf'\n",
    "        \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #spec2.assign_wcs.skip = False\n",
    "    spec2.bkg_subtract.skip = False   # 2D background subtraction!\n",
    "    spec2.bkg_subtract.save_combined_background = True\n",
    "    #spec2.flat_field.skip = False\n",
    "    #spec2.srctype.skip = False\n",
    "    #spec2.straylight.skip = False\n",
    "    #spec2.fringe.skip = False\n",
    "    #spec2.photom.skip = False\n",
    "    spec2.residual_fringe.skip = False\n",
    "    #spec2.cube_build.skip = False\n",
    "    #spec2.extract_1d.skip = False\n",
    "\n",
    "    # Run pixel replacement code to extrapolate values for otherwise bad pixels.\n",
    "    # This can help mitigate small 5-10% negative dips in spectra of bright sources.\n",
    "    spec2.pixel_replace.skip = False\n",
    "    spec2.pixel_replace.algorithm = 'mingrad'\n",
    "    \n",
    "    # This nocubes option allows us to skip the cube building and 1d spectral extraction for individual\n",
    "    # science data frames, but run it for the background data (as the 1d spectra are needed later\n",
    "    # for the master background step in Spec3)\n",
    "    if (nocubes):\n",
    "        spec2.cube_build.skip = True\n",
    "        spec2.extract_1d.skip = True\n",
    "    \n",
    "    # Some cube building options\n",
    "    spec2.cube_build.weighting='drizzle'\n",
    "    spec2.cube_build.coord_system='ifualign' # If aligning cubes with IFU axes instead of sky\n",
    "      \n",
    "    spec2.save_results = True\n",
    "    spec2(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for uncalibrated science slope files from the Detector1 pipeline\n",
    "sstring = det1_dir + 'jw*mirifu*ratemod.fits'\n",
    "ratefiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(ratefiles))\n",
    "    for ii in range(0,len(ratefiles)):\n",
    "        hdu=fits.open(ratefiles[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    ratefiles=ratefiles[indx]\n",
    "\n",
    "print('Found ' + str(len(ratefiles)) + ' input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad30204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for uncalibrated background slope files from the Detector1 pipeline\n",
    "sstring = det1_bgdir + 'jw*mirifu*ratemod.fits'\n",
    "bg_ratefiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(bg_ratefiles))\n",
    "    for ii in range(0,len(bg_ratefiles)):\n",
    "        hdu=fits.open(bg_ratefiles[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    bg_ratefiles=bg_ratefiles[indx]\n",
    "\n",
    "print('Found ' + str(len(bg_ratefiles)) + ' input files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94665449",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec2:\n",
    "    for (i, file) in enumerate(ratefiles):\n",
    "        # Make an association file that includes all of the different exposures\n",
    "        asnfile=os.path.join(output_dir, f'l2asn_{i}.json')\n",
    "                \n",
    "        # Look for uncalibrated background slope files from the Detector1 pipeline\n",
    "        sstring = det1_bgdir + 'jw*mirifu*rate.fits'\n",
    "        bg_ratefiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "        hdu = fits.open(file)\n",
    "        hdr = hdu[0].header\n",
    "        use_ch_i = hdr['CHANNEL']\n",
    "        use_bd_i = hdr['BAND']\n",
    "\n",
    "        # If desired, check that these are the band/channel to use\n",
    "        keep=np.zeros(len(bg_ratefiles))\n",
    "        for ii in range(0,len(bg_ratefiles)):\n",
    "            hdu=fits.open(bg_ratefiles[ii])\n",
    "            hdr=hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == use_ch_i)&(hdr['BAND'] == use_bd_i)):\n",
    "                keep[ii]=1\n",
    "            hdu.close()\n",
    "        indx=np.where(keep == 1)\n",
    "        bg_ratefiles=bg_ratefiles[indx]\n",
    "\n",
    "        writel2asn([file], bg_ratefiles, asnfile, 'Level2b')\n",
    "        runspec2(asnfile, spec2_dir, nocubes=True)\n",
    "else:\n",
    "    print('Skipping Spec2 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83655aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sstring = det1_bgdir + 'jw*mirifu*rate.fits'\n",
    "bg_ratefiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "if dospec2bg:\n",
    "    for file in bg_ratefiles:\n",
    "        runspec2(file, spec2_bgdir)\n",
    "else:\n",
    "    print('Skipping Spec2 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6415",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b351ac6",
   "metadata": {},
   "source": [
    "5.<font color='white'>-</font>Spec3 Pipeline: Default configuration (4 per-channel cubes)<a class=\"anchor\" id=\"spec3\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Here we'll run the Spec3 pipeline to produce a composite data cube from all dithered exposures.\n",
    "We will need to create an association file from all science and background data in order for the pipeline to use them appropriately.\n",
    "\n",
    "A word of caution: the data cubes created by the JWST pipeline are in SURFACE BRIGHTNESS units (MJy/steradian), not flux units.  What that means is that if you intend to sum spectra within an aperture you need to be sure to multiply by the pixel area in steradians first in order to get a spectrum in flux units (the PIXAR_SR keyword can be found in the SCI extension header).  This correction is already build into the pipeline Extract1D algorithm.\n",
    "\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "683839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl3 association file from an input list\n",
    "# Note that any background exposures have to be of type x1d.\n",
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    # Add background files to the association\n",
    "    nbg=len(bgfiles)\n",
    "    for ii in range(0,nbg):\n",
    "        asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files\n",
    "\n",
    "# Science Files need the cal.fits files\n",
    "sstring = spec2_dir + 'jw*mirifu*cal.fits'\n",
    "calfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(calfiles))\n",
    "    for ii in range(0,len(calfiles)):\n",
    "        hdu=fits.open(calfiles[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    calfiles=calfiles[indx]\n",
    "\n",
    "# Background Files need the x1d.fits files\n",
    "sstring = spec2_bgdir + 'jw*mirifu*x1d.fits'\n",
    "# sstring = spec2_bgdir + 'jw*mirifu*cal.fits'\n",
    "bgfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "print('Found ' + str(len(calfiles)) + ' science files to process')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2948ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "calfiles_dither1 = [cf for cf in calfiles if \"00001\" in cf]\n",
    "calfiles_dither2 = [cf for cf in calfiles if \"00002\" in cf]\n",
    "calfiles_dither3 = [cf for cf in calfiles if \"00003\" in cf]\n",
    "calfiles_dither4 = [cf for cf in calfiles if \"00004\" in cf]\n",
    "print(calfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9918b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an association file that includes all of the different exposures\n",
    "asnfile=os.path.join(output_dir, 'l3asn.json')\n",
    "if dospec3:\n",
    "    writel3asn(calfiles, bgfiles, asnfile, 'Level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80c035f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is designed to run on an association file\n",
    "def runspec3(filename):\n",
    "    # This initial setup is just to make sure that we get the latest parameter reference files\n",
    "    # pulled in for our files.  This is a temporary workaround to get around an issue with\n",
    "    # how this pipeline calling method works.\n",
    "    crds_config = Spec3Pipeline.get_config_from_reference(filename)\n",
    "    crds_config['steps']['extract_1d'].pop('ifu_covar_scale')   # ?????????????????????????\n",
    "    spec3 = Spec3Pipeline.from_config_section(crds_config)\n",
    "    \n",
    "    spec3.output_dir = spec3_dir\n",
    "    spec3.save_results = True\n",
    "    \n",
    "    # Cube building configuration options\n",
    "    # spec3.cube_build.output_file = 'mycube' # Custom output name\n",
    "    spec3.cube_build.output_type = 'band' # 'band', 'channel', or 'multi' type cube output\n",
    "    #spec3.cube_build.channel = '1' # Build everything from just channel 1 into a single cube (we could also choose '2','3','4', or 'ALL')\n",
    "    spec3.cube_build.weighting = 'drizzle' # 'emsm' or 'drizzle'\n",
    "    spec3.cube_build.coord_system = 'ifualign' # 'ifualign', 'skyalign', or 'internal_cal'\n",
    "    #spec3.cube_build.scalexy = 0.5 # Output cube spaxel scale (arcsec) if setting it by hand\n",
    "    #spec3.cube_build.scalew = 0.002 # Output cube voxel depth in wavelength if setting it by hand\n",
    "    #spec3.cube_build.ra_center = 65.0 # Force cube to be centered at this R.A.\n",
    "    #spec3.cube_build.dec_center = -35.0 # Force cube to be centered at this Decl.\n",
    "    #spec3.cube_build.cube_pa = 45.0 # Force cube to have this position angle\n",
    "    #spec3.cube_build.nspax_x = 61 # Require this number of spaxels in cube X direction\n",
    "    #spec3.cube_build.nspax_y = 61 # Require this number of spaxels in cube Y direction\n",
    "    #spec3.cube_build.wavemin = 4.8 # Custom minimum wavelength for the cube\n",
    "    #spec3.cube_build.wavemax = 6.3 # Custom maximum wavelength for the cube\n",
    "\n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #spec3.assign_mtwcs.skip = False\n",
    "\n",
    "    # spec3.master_background.skip = False\n",
    "\n",
    "    spec3.mrs_imatch.skip = True\n",
    "    spec3.outlier_detection.skip = False\n",
    "    spec3.outlier_detection.kernel_size = '7 7'\n",
    "    spec3.outlier_detection.threshold_percent = 95.0\n",
    "    spec3.outlier_detection.grow = 3\n",
    "    \n",
    "    spec3.cube_build.skip = False\n",
    "    spec3.extract_1d.skip = True\n",
    "    \n",
    "    # Cubepar overrides\n",
    "    #spec3.cube_build.override_cubepar = 'myfile.fits'\n",
    "\n",
    "    # Extract1D overrides and config options\n",
    "    #spec3.extract_1d.override_extract1d = 'myfile.asdf'\n",
    "    #spec3.extract_1d.override_apcorr = 'myfile.asdf'\n",
    "    spec3.extract_1d.ifu_autocen = True # Enable auto-centering of the extraction aperture\n",
    "    #spec3.extract_1d.center_xy=(20,20) # Override aperture location if desired\n",
    "    spec3.extract_1d.ifu_rfcorr = True # Turn on 1d residual fringe correction\n",
    "\n",
    "\n",
    "    spec3(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ca0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec3:\n",
    "    runspec3(asnfile)\n",
    "else:\n",
    "    print('Skipping Spec3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import CircularAperture, CircularAnnulus, SkyCircularAperture, SkyCircularAnnulus, aperture_photometry\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "dopost = True\n",
    "poststage3 = os.path.join(output_dir, 'poststage3/')\n",
    "if not os.path.exists(poststage3):\n",
    "    os.mkdir(poststage3)\n",
    "\n",
    "ch3med = fits.open(spec3_dir+'Level3_ch3-medium_s3d.fits')\n",
    "scihead = ch3med['SCI'].header\n",
    "wcs3med = WCS(ch3med['SCI'].header, naxis=2)\n",
    "scale3med = np.sqrt(ch3med['SCI'].header['PIXAR_A2'])\n",
    "wave3 = (np.arange(scihead['NAXIS3']) + scihead['CRPIX3'] - 1) * scihead['CDELT3'] + scihead['CRVAL3']\n",
    "wave3med = np.median(wave3)\n",
    "\n",
    "for channel in (1,2,3,4):\n",
    "    for band in ('short', 'medium', 'long'):\n",
    "        # Read in the pipe-produced Level 3 data cube for channel 3\n",
    "        cube = fits.open(spec3_dir+f'Level3_ch{channel}-{band}_s3d.fits')\n",
    "        sci  = cube['SCI'].data\n",
    "        err  = cube['ERR'].data\n",
    "        dq   = cube['DQ'].data\n",
    "        scihead = cube['SCI'].header\n",
    "        wcs = WCS(scihead, naxis=2)\n",
    "        wave = (np.arange(scihead['NAXIS3']) + scihead['CRPIX3'] - 1) * scihead['CDELT3'] + scihead['CRVAL3']\n",
    "        wavemed = np.median(wave)\n",
    "\n",
    "        # These will be useful\n",
    "        wts  = err**-2   # w = 1/sigma^2, useful for weighted averages etc.\n",
    "        scimasked = np.ma.masked_array(sci, mask = (dq>0)) # use DQ array to mask the science data\n",
    "        \n",
    "        # These define the pixel coordinates for the x and y center of the foreground lens,\n",
    "        # plus a radius based on the Einstein ring size. I (JS) get consistent results whether\n",
    "        # I get these using the cube's WCS information and ancillary ALMA data, or by collapsing\n",
    "        # the whole cube over wavelength and visibly seeing the lens+ring.\n",
    "\n",
    "        # Defined for channel 3MED\n",
    "        xpeak1, ypeak1, rout1 = 18.5, 17.5, 13.0\n",
    "        # xpeak2, ypeak2, rout2 = 26.5, 30.0, 7.0\n",
    "        # xpeak3, ypeak3, rout3 = 26.5, 4.0, 7.0\n",
    "        # xpeak4, ypeak4, rout4 = 14.5, 27.5, 6.0\n",
    "        # Transform to ch/band\n",
    "        xpeak1, ypeak1 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak1, ypeak1))\n",
    "        # xpeak2, ypeak2 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak2, ypeak2))\n",
    "        # xpeak3, ypeak3 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak3, ypeak3))\n",
    "        # xpeak4, ypeak4 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak4, ypeak4))\n",
    "        # Also rescale radii\n",
    "        rout1 *= scale3med / np.sqrt(scihead['PIXAR_A2']) * (0.033 * wavemed + 0.106) / (0.033 * wave3med + 0.106)\n",
    "        # rout1 = np.maximum(rout1, 2.5 / np.sqrt(scihead['PIXAR_A2']))\n",
    "        if channel in (1,2):\n",
    "            rout1 = 2.5 / np.sqrt(scihead['PIXAR_A2'])\n",
    "            xpeak1 += 3\n",
    "            ypeak1 -= 3\n",
    "        if channel == 4:\n",
    "            rout1 *= 1.2\n",
    "        # rout2 *= scale3med / np.sqrt(scihead['PIXAR_A2']) * (0.033 * wavemed + 0.106) / (0.033 * wave3med + 0.106)\n",
    "        # rout3 *= scale3med / np.sqrt(scihead['PIXAR_A2']) * (0.033 * wavemed + 0.106) / (0.033 * wave3med + 0.106)\n",
    "        # rout4 *= scale3med / np.sqrt(scihead['PIXAR_A2']) * (0.033 * wavemed + 0.106) / (0.033 * wave3med + 0.106)\n",
    "        \n",
    "        # Now we create a boolean mask, circular in shape, using the above params.\n",
    "        # This atrocious command masks a circle with radius=rout pix around the lens center\n",
    "        circmask1 = (np.linalg.norm(np.argwhere(sci[0]) - np.array([ypeak1,xpeak1]), axis=1) < rout1).reshape(sci[0].shape)\n",
    "        # circmask2 = (np.linalg.norm(np.argwhere(sci[0]) - np.array([ypeak2,xpeak2]), axis=1) < rout2).reshape(sci[0].shape)\n",
    "        # circmask3 = (np.linalg.norm(np.argwhere(sci[0]) - np.array([ypeak3,xpeak3]), axis=1) < rout3).reshape(sci[0].shape)\n",
    "        # circmask4 = (np.linalg.norm(np.argwhere(sci[0]) - np.array([ypeak4,xpeak4]), axis=1) < rout4).reshape(sci[0].shape)\n",
    "        \n",
    "        # Now use that (2D) circular mask to mask the source in all (3D) cube channels.\n",
    "        # Optionally I (JS) explored also masking some of the edge pixels which seem to\n",
    "        # be junk, but they also have high ERR / low weight so in weighted stats they matter little\n",
    "        sourcemask    = np.zeros(sci.shape, dtype=bool)\n",
    "        # sourcemask[:] = circmask1 | circmask2 | circmask3 | circmask4\n",
    "        sourcemask[:] = circmask1\n",
    "        if band == 'long' and channel == 3:\n",
    "            sourcemask[:, :, :5] = True\n",
    "        # sourcemask[:, :, :2] = True # mask edges\n",
    "        # sourcemask[:, :, 32] = True\n",
    "        \n",
    "        # Create a copy of the original (DQ masked) data but now include our\n",
    "        # additional mask from above\n",
    "        scisourcemask = scimasked.copy()\n",
    "        scisourcemask.mask += sourcemask\n",
    "        \n",
    "        # We will calculate the stripe template using a running average over the\n",
    "        # cube. There's an option to use either a straight average or a weighted average,\n",
    "        # so we'll just make both.\n",
    "        # These will be the smoothed cubes before stripe removal\n",
    "        cubeavg     = np.zeros(scisourcemask.shape)\n",
    "        cubeavg_wtd = np.zeros(scisourcemask.shape)\n",
    "        # These will contain the estimated stripe templates\n",
    "        cubebkg     = np.zeros(scisourcemask.shape)\n",
    "        cubebkg_wtd = np.zeros(scisourcemask.shape)\n",
    "        \n",
    "        # Setup for our background/stripe estimation. Stripes are coherent over tens\n",
    "        # of wavelength channels (remember cube has been \"3d drizzled\" so oversamples\n",
    "        # the detectors spectral resolution). After some experimentation I settled on\n",
    "        # a 25-channel running average.\n",
    "        chstep   = 25\n",
    "        halfstep = int((chstep-1)/2)\n",
    "        bkg_estimator = MedianBackground() # From photutils\n",
    "        \n",
    "        for chstart in np.arange(halfstep, sci.shape[0]):\n",
    "            cutout = np.ma.average(scimasked[chstart-halfstep:chstart+chstep+halfstep],axis=0)\n",
    "            cutout2= np.ma.average(scimasked[chstart-halfstep:chstart+chstep+halfstep],axis=0,weights=wts[chstart-halfstep:chstart+chstep+halfstep])\n",
    "\n",
    "            # Use photutils to estimate the 2D \"background\" (striping). The box_size\n",
    "            # parameter sets the shape of the stripe estimation. Here using (1,shape[1]/2)\n",
    "            # corresponds to fitting the \"background\" in a shape that is 1 row tall and\n",
    "            # half the x-pixels of the cube, which allows enough flexibility to account\n",
    "            # for the fact that the slices aren't perfectly x-aligned due to the slice\n",
    "            # curvature on the detector. Also exclude_percentile is very high here because\n",
    "            # in some rows most of the cube pixels are masked, where our circular source\n",
    "            # mask is at its widest point.\n",
    "            try:\n",
    "                bkg = Background2D(cutout,  box_size=(1,int(np.ceil(cutout.shape[1]))), mask=(cutout.mask | sourcemask[0]), \n",
    "                        filter_size=1, bkg_estimator=bkg_estimator, exclude_percentile=85.0)\n",
    "                bkg2= Background2D(cutout2, box_size=(1,int(np.ceil(cutout.shape[1]))), mask=(cutout2.mask | sourcemask[0]), \n",
    "                        filter_size=1, bkg_estimator=bkg_estimator, exclude_percentile=85.0)    \n",
    "\n",
    "                # Overwrite our blank cubes with data\n",
    "                cubeavg[chstart]     = cutout\n",
    "                cubeavg_wtd[chstart] = cutout2\n",
    "                cubebkg[chstart]     = bkg.background\n",
    "                cubebkg_wtd[chstart] = bkg2.background\n",
    "\n",
    "            except ValueError:\n",
    "\n",
    "                cubeavg[chstart]     = cutout\n",
    "                cubeavg_wtd[chstart] = cutout2\n",
    "                cubebkg[chstart]     = 0.\n",
    "                cubebkg_wtd[chstart] = 0.\n",
    "            \n",
    "        # The above fails near the edge channels at the start/end of the cube,\n",
    "        # pad those slices with the same background for convenience (and remember\n",
    "        # to ignore edge channels in later analysis)\n",
    "        cubeavg[0:halfstep]     = cubeavg[halfstep]\n",
    "        cubeavg_wtd[0:halfstep] = cubeavg_wtd[halfstep]\n",
    "        cubebkg[0:halfstep]     = cubebkg[halfstep]\n",
    "        cubebkg_wtd[0:halfstep] = cubebkg[halfstep]\n",
    "        cubeavg[-halfstep:]     = cubeavg[-halfstep]\n",
    "        cubeavg_wtd[-halfstep:] = cubeavg_wtd[-halfstep]\n",
    "        cubebkg[-halfstep:]     = cubebkg[-halfstep]\n",
    "        cubebkg_wtd[-halfstep:] = cubebkg[-halfstep]\n",
    "\n",
    "        # subtract the median background value so that we're not oversubtracting any real signal\n",
    "        for i in np.arange(sci.shape[0]):\n",
    "            cubebkg[i,:,:] = cubebkg[i,:,:] - np.nanmedian(cubebkg[i,:,:])\n",
    "            cubebkg_wtd[i,:,:] = cubebkg_wtd[i,:,:] - np.nanmedian(cubebkg_wtd[i,:,:])\n",
    "        \n",
    "        # Now write the wavelength-smoothed cubes and backgrounds to disk.\n",
    "        cubehdu = fits.PrimaryHDU(cubeavg,header=cube['SCI'].header)\n",
    "        cubehdu.writeto(poststage3+f'Level3_ch{channel}-{band}_s3d_runningavg.fits',overwrite=True)\n",
    "        cubehdu = fits.PrimaryHDU(cubeavg_wtd,header=cube['SCI'].header)\n",
    "        cubehdu.writeto(poststage3+f'Level3_ch{channel}-{band}_s3d_runningweightedavg.fits',overwrite=True)\n",
    "        bkghdu = fits.PrimaryHDU(cubebkg,header=cube['SCI'].header)\n",
    "        bkghdu.writeto(poststage3+f'Level3_ch{channel}-{band}_s3d_background.fits',overwrite=True)\n",
    "        bkghdu = fits.PrimaryHDU(cubebkg_wtd, header=cube['SCI'].header)\n",
    "        bkghdu.writeto(poststage3+f'Level3_ch{channel}-{band}_s3d_weightedbackground.fits',overwrite=True)\n",
    "        \n",
    "        # Finally, also write out the background-subtracted cubes to disk\n",
    "        bkgsub     = scimasked.data - cubebkg\n",
    "        bkgsub_wtd = scimasked.data - cubebkg_wtd\n",
    "        cubehdu = cube.copy()\n",
    "        cubehdu['SCI'].data = bkgsub\n",
    "        cubehdu.writeto(poststage3+f'Level3_ch{channel}-{band}_s3d_bkgsubtracted.fits',overwrite=True)\n",
    "        cubehdu = cube.copy()\n",
    "        cubehdu['SCI'].data = bkgsub_wtd\n",
    "        cubehdu.writeto(poststage3+f'Level3_ch{channel}-{band}_s3d_bkgwtdsubtracted.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109845c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in (1,2,3,4):\n",
    "    for band in ('short', 'medium', 'long'):\n",
    "        from matplotlib.patches import Ellipse\n",
    "\n",
    "        # The starting and stopping channels of the cubes we should plot\n",
    "        # You can play with these to see how the stripe properties vary\n",
    "        # as a function of cube width etc.\n",
    "        chstart, chstop = 500, 600\n",
    "\n",
    "        pipe = fits.open(spec3_dir+f'Level3_ch{channel}-{band}_s3d.fits')\n",
    "        back = fits.open(poststage3+f'Level3_ch{channel}-{band}_s3d_weightedbackground.fits')\n",
    "        bsub = fits.open(poststage3+f'Level3_ch{channel}-{band}_s3d_bkgwtdsubtracted.fits')\n",
    "\n",
    "        scihead = pipe['SCI'].header\n",
    "        wcs = WCS(scihead, naxis=2)\n",
    "        wave = (np.arange(scihead['NAXIS3']) + scihead['CRPIX3'] - 1) * scihead['CDELT3'] + scihead['CRVAL3']\n",
    "        wavemed = np.median(wave)\n",
    "\n",
    "        pipe = pipe['SCI'].data\n",
    "        back = back['SCI'].data\n",
    "        bsub = bsub['SCI'].data\n",
    "\n",
    "        # Defined for channel 3MED\n",
    "        xpeak1, ypeak1, rout1 = 18.5, 17.5, 13.0\n",
    "        # xpeak2, ypeak2, rout2 = 25.5, 12.5, 7.0\n",
    "        # xpeak3, ypeak3, rout3 = 15.5, 20.5, 7.0\n",
    "        # xpeak4, ypeak4, rout4 = 14.5, 27.5, 6.0\n",
    "        # Transform to ch/band\n",
    "        xpeak1, ypeak1 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak1, ypeak1))\n",
    "        # xpeak2, ypeak2 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak2, ypeak2))\n",
    "        # xpeak3, ypeak3 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak3, ypeak3))\n",
    "        # xpeak4, ypeak4 = wcs.world_to_pixel(wcs3med.pixel_to_world(xpeak4, ypeak4))\n",
    "        # Also rescale radii\n",
    "        rout1 *= scale3med / np.sqrt(scihead['PIXAR_A2']) * (0.033 * wavemed + 0.106) / (0.033 * wave3med + 0.106)\n",
    "        # rout1 = np.maximum(rout1, 2.5 / np.sqrt(scihead['PIXAR_A2']))\n",
    "        if channel in (1,2):\n",
    "            rout1 = 2.5 / np.sqrt(scihead['PIXAR_A2'])\n",
    "            xpeak1 += 3\n",
    "            ypeak1 -= 3\n",
    "        if channel == 4:\n",
    "            rout1 *= 1.2\n",
    "        # rout2 *= scale3med / np.sqrt(scihead['PIXAR_A2'])\n",
    "        # rout3 *= scale3med / np.sqrt(scihead['PIXAR_A2'])\n",
    "        # rout4 *= scale3med / np.sqrt(scihead['PIXAR_A2']) * (0.033 * wavemed + 0.106) / (0.033 * wave3med + 0.106)\n",
    "        \n",
    "        # Edge pixels are flagged in original cube, but not in mine (we ignored the\n",
    "        # DQ arrays when writing back to disk), flag them here for consistency\n",
    "        back[:,:1,:]  = 0.\n",
    "        back[:,-1:,:] = 0.\n",
    "        back[:,:,:1]  = 0.\n",
    "        back[:,:,-1:] = 0.\n",
    "        bsub[:,:1,:]  = 0.\n",
    "        bsub[:,-1:,:] = 0.\n",
    "        bsub[:,:,:2]  = 0.\n",
    "        bsub[:,:,-2:] = 0.\n",
    "        \n",
    "        # Average the cube over the specified channel range\n",
    "        pipedata = np.ma.average(pipe[chstart:chstop], axis=0)\n",
    "        backdata = np.ma.average(back[chstart:chstop], axis=0)\n",
    "        bsubdata = np.ma.average(bsub[chstart:chstop], axis=0)\n",
    "        \n",
    "        f,axarr = plt.subplots(figsize=(7.5,3),nrows=1,ncols=3)\n",
    "        f.subplots_adjust(left=0.01,right=0.99,bottom=0.01,top=0.9,hspace=0,wspace=0)\n",
    "        \n",
    "        axarr[0].imshow(pipedata, origin='lower', cmap='jet', vmin=-3., vmax=20.0)\n",
    "        axarr[1].imshow(backdata, origin='lower', cmap='jet', vmin=-3., vmax=20.0)\n",
    "        axarr[2].imshow(bsubdata, origin='lower', cmap='jet', vmin=-3., vmax=20.0)\n",
    "        \n",
    "        axarr[0].set_title('Original Pipeline',fontsize='large')\n",
    "        axarr[1].set_title('Estimated Striping',fontsize='large')\n",
    "        axarr[2].set_title('Stripes Removed',fontsize='large')\n",
    "        \n",
    "        for ax in axarr:\n",
    "            # note we actually shift the drawing of the masked regions bc matplotlib\n",
    "            # defines pixels differently than numpy (it's a half-pixel thing as always)\n",
    "            mask = Ellipse(xy=(xpeak1+0.5,ypeak1+0.5),width=2*rout1,height=2*rout1,angle=0.,ec='grey',fc='None')\n",
    "            ax.add_artist(mask)\n",
    "            # mask = Ellipse(xy=(xpeak2+0.5,ypeak2+0.5),width=2*rout2,height=2*rout2,angle=0.,ec='grey',fc='None')\n",
    "            # ax.add_artist(mask)\n",
    "            # mask = Ellipse(xy=(xpeak3+0.5,ypeak3+0.5),width=2*rout3,height=2*rout3,angle=0.,ec='grey',fc='None')\n",
    "            # ax.add_artist(mask)\n",
    "            # mask = Ellipse(xy=(xpeak4+0.5,ypeak4+0.5),width=2*rout4,height=2*rout4,angle=0.,ec='grey',fc='None')\n",
    "            # ax.add_artist(mask)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e2553",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d16ec",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://www.stsci.edu/~dlaw/stsci_logo.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwst_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
